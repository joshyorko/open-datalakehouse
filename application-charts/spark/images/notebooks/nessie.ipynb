{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4110b0b0-c33c-4fd7-a173-ef0002ccf103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "\n",
    "# Set AWS credentials as environment variables\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = ''\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = ''\n",
    "\n",
    "# Nessie and AWS configurations\n",
    "NESSIE_URI = \"http://nessie:19120/api/v1\"\n",
    "WAREHOUSE = \"s3a://warehouse/\"\n",
    "AWS_S3_ENDPOINT = \"http://dremio-minio:9000\"\n",
    "AWS_REGION = \"us-east-1\"  # Change this to the region of your S3 bucket\n",
    "\n",
    "# Path to the PostgreSQL JDBC driver jar\n",
    "jdbc_driver_path = \"postgresql-42.2.23.jar\"\n",
    "\n",
    "# Initialize SparkConf with updated packages and configurations\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "    .setAppName(\"Iceberg Partitioned Data Write\")\n",
    "    .set(\"spark.jars\", jdbc_driver_path)  # Include the JDBC driver\n",
    "    .set(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.91.3,software.amazon.awssdk:bundle:2.17.81,org.apache.hadoop:hadoop-aws:3.3.1\")  # Include Iceberg, Nessie, AWS SDK, and Hadoop AWS packages\n",
    "    .set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\")  # Corrected Spark session extensions\n",
    "    .set(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .set(\"spark.sql.catalog.nessie.uri\", NESSIE_URI)\n",
    "    .set(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .set(\"spark.sql.catalog.nessie.authentication.type\", \"NONE\")\n",
    "    .set(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .set(\"spark.sql.catalog.nessie.s3.endpoint\", AWS_S3_ENDPOINT)\n",
    "    .set(\"spark.sql.catalog.nessie.warehouse\", WAREHOUSE)\n",
    "    .set(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", AWS_S3_ENDPOINT)\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", os.environ['AWS_ACCESS_KEY_ID'])\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint.region\", AWS_REGION)\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    ")\n",
    "\n",
    "# Initialize Spark session with the configured SparkConf\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Database connection parameters\n",
    "db_user = \"jyorko\"\n",
    "db_password = \"jyorkopassword\"\n",
    "db_host = 'db-postgresql'\n",
    "db_port = \"5432\"\n",
    "db_name = \"daily_work\"\n",
    "db_url = f\"jdbc:postgresql://{db_host}:{db_port}/{db_name}\"\n",
    "db_properties = {\n",
    "    \"user\": db_user,\n",
    "    \"password\": db_password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Create a Table\n",
    "spark.sql(\"CREATE TABLE nessie.names2 (name STRING) USING iceberg;\").show()\n",
    "\n",
    "# Insert Some Data\n",
    "spark.sql(\"INSERT INTO nessie.names2 VALUES ('Alex Merced'), ('Joshua Yorko'), ('Jason Hughes'), ('Ron DeMena')\").show()\n",
    "\n",
    "# Query the Data\n",
    "spark.sql(\"SELECT * FROM nessie.names;\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ff9ab1-b02f-4621-8642-083309ff007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import pyspark\n",
    "\n",
    "# Set AWS credentials as environment variables\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = ''\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = ''\n",
    "\n",
    "# Nessie and AWS configurations\n",
    "NESSIE_URI = \"http://nessie:19120/api/v1\"\n",
    "WAREHOUSE = \"s3a://warehouse/\"\n",
    "AWS_S3_ENDPOINT = \"http://dremio-minio:9000\"\n",
    "AWS_REGION = \"us-east-1\"  # Change this to the region of your S3 bucket\n",
    "\n",
    "# Database connection parameters\n",
    "db_user = \"jyorko\"\n",
    "db_password = \"jyorkopassword\"\n",
    "db_host = 'db-postgresql'\n",
    "db_port = \"5432\"\n",
    "db_name = \"daily_work\"\n",
    "db_url = f\"jdbc:postgresql://{db_host}:{db_port}/{db_name}\"\n",
    "db_properties = {\n",
    "    \"user\": db_user,\n",
    "    \"password\": db_password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Path to the PostgreSQL JDBC driver jar\n",
    "jdbc_driver_path = \"postgresql-42.2.23.jar\"\n",
    "\n",
    "# Initialize SparkConf with updated packages and configurations\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "    .setAppName(\"Postgres to Nessie\")\n",
    "    .set(\"spark.jars\", jdbc_driver_path)  # Include the JDBC driver\n",
    "    .set(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.91.3,software.amazon.awssdk:bundle:2.17.81,org.apache.hadoop:hadoop-aws:3.3.1\")  # Include Iceberg, Nessie, AWS SDK, and Hadoop AWS packages\n",
    "    .set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\")  # Corrected Spark session extensions\n",
    "    .set(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .set(\"spark.sql.catalog.nessie.uri\", NESSIE_URI)\n",
    "    .set(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "    .set(\"spark.sql.catalog.nessie.authentication.type\", \"NONE\")\n",
    "    .set(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    .set(\"spark.sql.catalog.nessie.s3.endpoint\", AWS_S3_ENDPOINT)\n",
    "    .set(\"spark.sql.catalog.nessie.warehouse\", WAREHOUSE)\n",
    "    .set(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", AWS_S3_ENDPOINT)\n",
    "    .set(\"spark.hadoop.fs.s3a.access.key\", os.environ['AWS_ACCESS_KEY_ID'])\n",
    "    .set(\"spark.hadoop.fs.s3a.secret.key\", os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint.region\", AWS_REGION)\n",
    "    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .set(\"spark.executor.memory\", \"4g\")  # Increase executor memory\n",
    "    .set(\"spark.driver.memory\", \"4g\")  # Increase driver memory\n",
    "    .set(\"spark.sql.debug.maxToStringFields\", \"1000\")  # Adjust the maximum number of fields to be displayed\n",
    ")\n",
    "\n",
    "# Initialize Spark session with the configured SparkConf\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "def create_and_upload_table_to_nessie(table_name, db_url, db_properties):\n",
    "    try:\n",
    "        # Read distinct dates from PostgreSQL\n",
    "        distinct_dates_df = spark.read.jdbc(url=db_url, table=f\"(SELECT DISTINCT date FROM {table_name}) AS dates\", properties=db_properties)\n",
    "        distinct_dates = [row[\"date\"] for row in distinct_dates_df.collect()]\n",
    "\n",
    "        # Define the Iceberg table name in Nessie\n",
    "        iceberg_table_name = f\"nessie.{table_name}\"\n",
    "\n",
    "        # Create the Iceberg table schema in Nessie if it doesn't exist\n",
    "        # Create an empty DataFrame with the correct schema to use for table creation\n",
    "        source_df = spark.read.jdbc(url=db_url, table=table_name, properties=db_properties)\n",
    "        source_df.limit(0).writeTo(iceberg_table_name).using(\"iceberg\").createOrReplace()\n",
    "\n",
    "        for date in distinct_dates:\n",
    "            print(f\"Processing date: {date}\")\n",
    "\n",
    "            # Read data for the specific date from PostgreSQL\n",
    "            date_df = spark.read.jdbc(url=db_url, table=f\"(SELECT * FROM {table_name} WHERE date = '{date}') AS date_table\", properties=db_properties)\n",
    "\n",
    "            # Write data to Iceberg table in Nessie for the specific date\n",
    "            date_df.writeTo(iceberg_table_name).append()\n",
    "\n",
    "        print(f\"Successfully wrote table {table_name} to Nessie\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process table {table_name}: {e}\")\n",
    "\n",
    "# List of tables to process\n",
    "tables = [\"hdr\"]  # Add other table names as needed\n",
    "\n",
    "for table_name in tables:\n",
    "    # Create and upload the table data to Nessie\n",
    "    create_and_upload_table_to_nessie(table_name, db_url, db_properties)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0223807-fa5d-47bb-8256-92cad5bcc06e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
