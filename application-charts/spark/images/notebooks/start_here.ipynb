{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "import tarfile\n",
    "from datetime import datetime, timedelta\n",
    "from utils.utils import configure_spark\n",
    "\n",
    "def create_iceberg_table_if_not_exists(spark, table_name, df, partition_column='DATE'):\n",
    "    \"\"\"Create an Iceberg table if it doesn't exist.\"\"\"\n",
    "    schema = ', '.join([f'`{field.name}` {field.dataType.simpleString()}' for field in df.schema.fields])\n",
    "    create_table_query = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            {schema}\n",
    "        )\n",
    "        USING iceberg\n",
    "        PARTITIONED BY ({partition_column})\n",
    "    \"\"\"\n",
    "    spark.sql(create_table_query)\n",
    "\n",
    "def query_and_write_to_local_table(spark, nessie_table_name, date, local_table_name):\n",
    "    \"\"\"Query data from Nessie table for a given date and write it to the local Iceberg table if it doesn't already exist.\"\"\"\n",
    "    try:\n",
    "        existing_data = spark.sql(f\"SELECT * FROM {local_table_name} WHERE DATE = '{date}'\")\n",
    "        if existing_data.count() > 0:\n",
    "            print(f\"Data for {date} already exists in {local_table_name}. Skipping append.\")\n",
    "            return\n",
    "    except Exception as e:\n",
    "        print(f\"No existing data found for {date} in {local_table_name}. Proceeding with append.\")\n",
    "\n",
    "    query = f\"SELECT * FROM nessie.{nessie_table_name} WHERE DATE = '{date}';\"\n",
    "    df = spark.sql(query)\n",
    "    create_iceberg_table_if_not_exists(spark, local_table_name, df)\n",
    "    df.writeTo(local_table_name).append()\n",
    "    print(f\"Successfully appended data for {date} to {local_table_name}.\")\n",
    "\n",
    "def untar_latest_iceberg_warehouse(iceberg_warehouse_path):\n",
    "    \"\"\"Untar the most recent iceberg_warehouse tar.gz file if the directory doesn't exist.\"\"\"\n",
    "    base_name = os.path.basename(iceberg_warehouse_path)\n",
    "    directory = os.path.dirname(iceberg_warehouse_path) or '.'  # Default to current directory if no parent\n",
    "\n",
    "    try:\n",
    "        # hdrusted to match the prefix pattern based on date\n",
    "        tar_files = [f for f in os.listdir(directory) if f.endswith(f'{base_name}.tar.gz')]\n",
    "    \n",
    "        if not tar_files:\n",
    "            print(f\"No tar.gz file found for {iceberg_warehouse_path}. Proceeding without un-tarring.\")\n",
    "            return\n",
    "\n",
    "        latest_tar_file = max(tar_files, key=lambda f: os.path.getctime(os.path.join(directory, f)))\n",
    "        latest_tar_path = os.path.join(directory, latest_tar_file)\n",
    "    \n",
    "        if not os.path.exists(iceberg_warehouse_path):\n",
    "            with tarfile.open(latest_tar_path, \"r:gz\") as tar:\n",
    "                tar.extractall(path=directory)\n",
    "            print(f\"Un-tarred {latest_tar_path} successfully.\")\n",
    "        else:\n",
    "            print(f\"Directory {iceberg_warehouse_path} already exists. Skipping un-tarring.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to un-tar {latest_tar_path}. Error: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "def tar_iceberg_warehouse(iceberg_warehouse_path, date_prefix):\n",
    "    \"\"\"Tar the iceberg_warehouse directory with a date prefix.\"\"\"\n",
    "    tar_path = f\"{date_prefix}_iceberg_warehouse.tar.gz\"\n",
    "    try:\n",
    "        with tarfile.open(tar_path, \"w:gz\") as tar:\n",
    "            tar.add(iceberg_warehouse_path, arcname=os.path.basename(iceberg_warehouse_path))\n",
    "        print(f\"Tarred {iceberg_warehouse_path} to {tar_path} successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to tar {iceberg_warehouse_path}. Error: {e}\")\n",
    "\n",
    "def process_dates(spark, nessie_table_name, local_table_name, dates):\n",
    "    \"\"\"Process a list of dates to query from Nessie and write to both local and Nessie Iceberg tables.\"\"\"\n",
    "    for date in dates:\n",
    "        query_and_write_to_local_table(spark, nessie_table_name, date, local_table_name)\n",
    "        # read_local_and_write_to_nessie(spark, local_table_name, nessie_table_name)\n",
    "iceberg_warehouse_path = \"iceberg_warehouse\"\n",
    "\n",
    "untar_latest_iceberg_warehouse(iceberg_warehouse_path)\n",
    "\n",
    "if not os.path.exists(iceberg_warehouse_path):\n",
    "    os.makedirs(iceberg_warehouse_path, exist_ok=True)\n",
    "\n",
    "spark = configure_spark('minio', 'main')\n",
    "\n",
    "tables_to_process = []\n",
    "\n",
    "# Detect today's date\n",
    "today_date = datetime.today().date()\n",
    "#date = today_date - timedelta(days=1)\n",
    "print(today_date)\n",
    "\n",
    "# Process each table for today's date\n",
    "for table in tables_to_process:\n",
    "    nessie_table_name = table\n",
    "    local_table_name = f\"spark_catalog.default.{table}\"\n",
    "    process_dates(spark, nessie_table_name, local_table_name, [today_date])\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n",
    "tar_iceberg_warehouse(iceberg_warehouse_path, today_date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Spark Session and Process Tables\n",
    "Initialize a Spark session and perform any necessary table processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Iceberg Table Locally\n",
    "Run queries against the Iceberg table stored locally to retrieve data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import configure_spark\n",
    "\n",
    "\n",
    "spark = configure_spark('minio','main')\n",
    "\n",
    "# Define the Iceberg table name\n",
    "table_name = \"spark_catalog.default.{table}\"  # Make sure this matches the table you've created and written to\n",
    "\n",
    "# Query the Iceberg table locally\n",
    "df = spark.sql(f\"SELECT * FROM {table_name}\")\n",
    "\n",
    "# Show the results\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Data from Dremio and Load into DuckDB\n",
    "Query data from Dremio and load the results into a DuckDB table for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dremio_simple_query.connect import get_token, DremioConnection\n",
    "import duckdb\n",
    "\n",
    "# URL to Login Endpoint\n",
    "login_endpoint = \"http://dremio/apiv2/login\"\n",
    "\n",
    "# Payload for Login\n",
    "payload = {\n",
    "    \"userName\": \"\",\n",
    "    \"password\": \"\"\n",
    "}\n",
    "\n",
    "# Get token from API\n",
    "token = get_token(uri=login_endpoint, payload=payload)\n",
    "\n",
    "\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# URL Dremio Software Flight Endpoint\n",
    "arrow_endpoint = \"grpc://dremio:32010\"\n",
    "\n",
    "# Establish Client\n",
    "dremio = DremioConnection(token, arrow_endpoint)\n",
    "\n",
    "table_name = \"\"\n",
    "# Query data from Dremio and load it into DuckDB\n",
    "duck_rel = dremio.toPandas(\n",
    "    f\"\"\"\n",
    "    SELECT * FROM nessie.{table_name};\n",
    "    \"\"\"\n",
    ")\n",
    "duck_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Iceberg Table and Save Results to DuckDB Table\n",
    "Query the Iceberg table and save the results into a DuckDB table for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "table_name = \"\"\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect('iceberg_data.duckdb')\n",
    "\n",
    "# Install and load the Iceberg extension\n",
    "con.execute(\"INSTALL iceberg;\")\n",
    "con.execute(\"LOAD iceberg;\")\n",
    "\n",
    "# Query the Iceberg table and save the results to a DuckDB table\n",
    "con.execute(f\"\"\"\n",
    "    CREATE TABLE {table_name} AS\n",
    "    SELECT *\n",
    "    FROM iceberg_scan('iceberg_warehouse/default/{table_name}', allow_moved_paths = true) ;\n",
    "\"\"\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
