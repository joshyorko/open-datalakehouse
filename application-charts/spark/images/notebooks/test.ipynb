{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Data to Iceberg Table using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "from utils.utils import configure_spark, get_db_tables\n",
    "# Define the path for the Iceberg warehouse\n",
    "iceberg_warehouse_path = \"iceberg_warehouse\"  # Change this to your desired path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spark = configure_spark('minio','main')\n",
    "\n",
    "\n",
    "local_table_name = \"spark_catalog.default.employees\"\n",
    "df = spark.read.format(\"iceberg\").table(local_table_name)\n",
    "\n",
    "\n",
    "\n",
    "table_name = \"nessie.employees\"\n",
    "\n",
    "schema = ', '.join([f'{field.name} {field.dataType.simpleString()}' for field in df.schema.fields])\n",
    "\n",
    "def create_iceberg_table_if_not_exists(spark, table_name, df, partition_column):\n",
    "    \"\"\"Create an Iceberg table if it doesn't exist.\"\"\"\n",
    "    schema = ', '.join([f'`{field.name}` {field.dataType.simpleString()}' for field in df.schema.fields])\n",
    "    create_table_query = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            {schema}\n",
    "        )\n",
    "        USING iceberg\n",
    "        PARTITIONED BY ({partition_column})\n",
    "    \"\"\"\n",
    "    spark.sql(create_table_query)\n",
    "\n",
    "# Write the DataFrame to the existing Iceberg table partitioned by the 'DATE' column\n",
    "create_iceberg_table_if_not_exists(spark, table_name, df, 'companyName')\n",
    "df.write.format(\"iceberg\").mode(\"overwrite\") \\\n",
    "    .save(table_name)\n",
    "\n",
    "print(f\"Data has been written to Iceberg table at {iceberg_warehouse_path}/default/{table_name}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Data with Spark and Iceberg\n",
    "\n",
    "This script demonstrates how to process data using Spark and Iceberg. It includes functions to create Iceberg tables, query data from a Parquet file, and write data to both local and Nessie-managed Iceberg tables.\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. **Configure Spark**: Set up a Spark session using a custom configuration function `configure_spark`.\n",
    "2. **Create Iceberg Table**: Define a function to create an Iceberg table if it doesn't already exist.\n",
    "3. **Query and Write to Local Table**: Define a function to read data from a Parquet file and write it to a local Iceberg table.\n",
    "4. **Read Local and Write to Nessie**: Define a function to read data from a local Iceberg table and write it to a Nessie-managed Iceberg table.\n",
    "5. **Process Data**: Define a function to process data by querying from a Parquet file and writing to both local and Nessie Iceberg tables.\n",
    "6. **Execute Processing**: Set up paths and table names, then execute the data processing function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "from utils.utils import configure_spark\n",
    "\n",
    "def create_iceberg_table_if_not_exists(spark, table_name, df, partition_column):\n",
    "    \"\"\"Create an Iceberg table if it doesn't exist.\"\"\"\n",
    "    schema = ', '.join([f'`{field.name}` {field.dataType.simpleString()}' for field in df.schema.fields])\n",
    "    create_table_query = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            {schema}\n",
    "        )\n",
    "        USING iceberg\n",
    "        PARTITIONED BY ({partition_column})\n",
    "    \"\"\"\n",
    "    spark.sql(create_table_query)\n",
    "\n",
    "def query_and_write_to_local_table(spark, parquet_path, local_table_name, partition_column):\n",
    "    \"\"\"Query data from Nessie table for a given date and write it to the local Iceberg table.\"\"\"\n",
    "    df = spark.read.parquet(parquet_path)\n",
    "    create_iceberg_table_if_not_exists(spark, local_table_name, df, partition_column)\n",
    "    df.writeTo(local_table_name).append()\n",
    "\n",
    "def read_local_and_write_to_nessie(spark, local_table_name, nessie_table_name, partition_column):\n",
    "    \"\"\"Read data from local Iceberg table and write it to the Nessie-managed Iceberg table.\"\"\"\n",
    "    local_df = spark.read.format(\"iceberg\").table(local_table_name)\n",
    "    create_iceberg_table_if_not_exists(spark, nessie_table_name, local_df, partition_column)\n",
    "    local_df.writeTo(nessie_table_name).append()\n",
    "\n",
    "def process_data(spark, parquet_path, local_table_name,nessie_table_name,partition_column):\n",
    "    \"\"\"Process a list of dates to query from Nessie and write to both local and Nessie Iceberg tables.\"\"\"\n",
    "  \n",
    "    query_and_write_to_local_table(spark, parquet_path, local_table_name,partition_column)\n",
    "    #read_local_and_write_to_nessie(spark, local_table_name, nessie_table_name, partition_column)\n",
    "\n",
    "\n",
    "\n",
    "# Define the path for the Iceberg warehouse\n",
    "iceberg_warehouse_path = \"iceberg_warehouse\"\n",
    "os.makedirs(iceberg_warehouse_path, exist_ok=True)\n",
    "# Initialize Spark session with the configured SparkConf\n",
    "#spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark = configure_spark('minio','main')\n",
    "\n",
    "# Define the Nessie and local table names\n",
    "parquet_path = 'scripts/python_go_backups/employees_go.parquet'\n",
    "nessie_table_name = \"nessie.employees\"\n",
    "local_table_name = \"spark_catalog.default.employees\"\n",
    "partition_column = 'companyName'\n",
    "\n",
    "process_data(spark, parquet_path, local_table_name,nessie_table_name, partition_column)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying Employee Count by Company using Spark and Iceberg\n",
    "\n",
    "This script demonstrates how to configure a Spark session, read an Iceberg table into a Spark DataFrame, register it as a temporary view, and perform a query to count employees grouped by company name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.utils import configure_spark\n",
    "\n",
    "local_table_name = \"spark_catalog.default.employees\"\n",
    "spark = configure_spark('minio','main')\n",
    "\n",
    "# Read the Iceberg table into a DataFrame\n",
    "local_df = spark.read.format(\"iceberg\").table(local_table_name)\n",
    "\n",
    "# Register the DataFrame as a temporary view\n",
    "local_df.createOrReplaceTempView(\"employees_view\")\n",
    "\n",
    "# Perform the query using Spark SQL\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT companyName, COUNT(*) as employee_count\n",
    "    FROM employees_view\n",
    "    GROUP BY companyName\n",
    "\"\"\")\n",
    "\n",
    "# Show the results\n",
    "result_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDB and Iceberg Integration\n",
    "\n",
    "This script demonstrates how to connect to a DuckDB database, install and load the Iceberg extension, and query an Iceberg table to save the results into a DuckDB table.\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. **Connect to DuckDB**: Establish a connection to a DuckDB database file named `iceberg_data.duckdb`.\n",
    "2. **Install and Load Iceberg Extension**: Install and load the Iceberg extension to enable querying Iceberg tables.\n",
    "3. **Query Iceberg Table**: Execute a query to scan an Iceberg table located at `iceberg_warehouse/default/employees` and save the results into a DuckDB table named `employees`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect('iceberg_data.duckdb')\n",
    "\n",
    "# Install and load the Iceberg extension\n",
    "con.execute(\"INSTALL iceberg;\")\n",
    "con.execute(\"LOAD iceberg;\")\n",
    "\n",
    "# Query the Iceberg table and save the results to a DuckDB table\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE employees AS\n",
    "    SELECT *\n",
    "    FROM iceberg_scan('iceberg_warehouse/default/employees', allow_moved_paths = true);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying Employee Count by Company\n",
    "\n",
    "This script demonstrates how to connect to a DuckDB database, execute a query to get the count of employees grouped by company name, and convert the result into a pandas DataFrame.\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. **Connect to DuckDB**: Establish a connection to a DuckDB database file named `iceberg_data.duckdb`.\n",
    "2. **Execute Query**: Run a SQL query to count the number of employees grouped by `companyName` and order the results by `employee_count` in descending order.\n",
    "3. **Convert to DataFrame**: Convert the query result into a pandas DataFrame for further analysis or manipulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect('iceberg_data.duckdb')\n",
    "\n",
    "# Query to get the count of employees by companyName and convert it to a pandas DataFrame\n",
    "df = con.execute(\"\"\"\n",
    "    SELECT companyName, COUNT(*) as employee_count\n",
    "    FROM employees\n",
    "    GROUP BY companyName ORDER BY employee_count DESC;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
