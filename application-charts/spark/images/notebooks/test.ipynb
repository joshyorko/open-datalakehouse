{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_spark(provider, branch):\n",
    "    \"\"\"Configure and initialize Spark session.\"\"\"\n",
    "    configure_environment(provider)\n",
    "    NESSIE_URI = \"http://a2e39ba23e81f4c4aaed676e345f1fc9-1136246907.us-west-2.elb.amazonaws.com:19120/api/v1\"\n",
    "    WAREHOUSE = \"s3a://warehouse/\"\n",
    "    AWS_S3_ENDPOINT = \"http://a8c78c2e92dd442ff93f5d247b6a2cc7-1221119726.us-west-2.elb.amazonaws.com:9000\"\n",
    "    AWS_REGION = \"us-east-1\"\n",
    "    iceberg_warehouse_path = \"iceberg_warehouse\"\n",
    "\n",
    "    conf = (\n",
    "        pyspark.SparkConf()\n",
    "        .setAppName(\"Iceberg Partitioned Data Write\")\n",
    "        .set(\n",
    "            \"spark.jars.packages\",\n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.91.3,software.amazon.awssdk:bundle:2.17.81,org.apache.hadoop:hadoop-aws:3.3.1\",\n",
    "        )\n",
    "        .set(\n",
    "            \"spark.sql.extensions\",\n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\",\n",
    "        )\n",
    "        .set(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .set(\"spark.sql.catalog.nessie.uri\", NESSIE_URI)\n",
    "        .set(\"spark.sql.catalog.nessie.ref\", branch)\n",
    "        .set(\"spark.sql.catalog.nessie.authentication.type\", \"NONE\")\n",
    "        .set(\n",
    "            \"spark.sql.catalog.nessie.catalog-impl\",\n",
    "            \"org.apache.iceberg.nessie.NessieCatalog\",\n",
    "        )\n",
    "        .set(\"spark.sql.catalog.nessie.s3.endpoint\", AWS_S3_ENDPOINT)\n",
    "        .set(\"spark.sql.catalog.nessie.warehouse\", WAREHOUSE)\n",
    "        .set(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint\", AWS_S3_ENDPOINT)\n",
    "        .set(\"spark.hadoop.fs.s3a.access.key\", os.environ[\"AWS_ACCESS_KEY_ID\"])\n",
    "        .set(\"spark.hadoop.fs.s3a.secret.key\", os.environ[\"AWS_SECRET_ACCESS_KEY\"])\n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint.region\", AWS_REGION)\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .set(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .set(\"spark.sql.catalog.local.type\", \"hadoop\")\n",
    "        .set(\"spark.sql.catalog.local.warehouse\", iceberg_warehouse_path)\n",
    "        .set(\"spark.executor.memory\", \"8g\")\n",
    "        .set(\"spark.driver.memory\", \"8g\")\n",
    "        .set(\"spark.executor.instances\", \"5\")\n",
    "        .set(\"spark.local.dir\", \"/tmp/spark-temp\")\n",
    "    )\n",
    "    return SparkSession.builder.config(conf=conf).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "\n",
    "# Define the path for the Iceberg warehouse\n",
    "iceberg_warehouse_path = \"iceberg_warehouse\"  # Change this to your desired path\n",
    "table = input(\"Enter the table name: \")\n",
    "\n",
    "os.makedirs(iceberg_warehouse_path, exist_ok=True)\n",
    "\n",
    "# Spark configuration\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "    .setAppName(\"Iceberg Partitioned Data Write\")\n",
    "    .set(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,software.amazon.awssdk:bundle:2.17.81,org.apache.hadoop:hadoop-aws:3.3.1\",\n",
    "    )\n",
    "    .set(\n",
    "        \"spark.sql.extensions\",\n",
    "        \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    )\n",
    "    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .set(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\")\n",
    "    .set(\"spark.sql.catalog.spark_catalog.warehouse\", iceberg_warehouse_path)\n",
    "    .set(\"spark.executor.memory\", \"8g\")\n",
    "    .set(\"spark.driver.memory\", \"8g\")\n",
    "    .set(\"spark.executor.instances\", \"5\")\n",
    "    .set(\"spark.local.dir\", \"/tmp/spark-temp\")\n",
    ")\n",
    "\n",
    "# Initialize Spark session with the configured SparkConf\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Path to the Parquet file\n",
    "parquet_path = \"scripts/python_go_backups/employees_go.parquet\"  # Change this to the actual path\n",
    "\n",
    "# Read the Parquet file into a Spark DataFrame\n",
    "df = spark.read.parquet(parquet_path)\n",
    "\n",
    "# Convert column headers to uppercase (optional)\n",
    "df = df.select([col(column).alias(column.upper()) for column in df.columns])\n",
    "\n",
    "# Define the table name\n",
    "table_name = f\"spark_catalog.default.{table}\"\n",
    "\n",
    "# Define the table schema with backticks around column names\n",
    "schema = ', '.join([f'`{field.name}` {field.dataType.simpleString()}' for field in df.schema.fields])\n",
    "\n",
    "# Create the Iceberg table if it doesn't exist\n",
    "create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        {schema}\n",
    "    )\n",
    "    USING iceberg\n",
    "\n",
    "    PARTITIONED BY (COMPANYNAME)\n",
    "  \n",
    "\"\"\"\n",
    "spark.sql(create_table_query)\n",
    "\n",
    "# Write the DataFrame to the existing Iceberg table partitioned by the 'DATE' column\n",
    "df.writeTo(table_name).append()\n",
    "\n",
    "print(f\"Data has been written to Iceberg table at {iceberg_warehouse_path}/default/allocation_iceberg\"/{table})\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "from utils.utils import configure_spark, get_db_tables\n",
    "# Define the path for the Iceberg warehouse\n",
    "iceberg_warehouse_path = \"iceberg_warehouse\"  # Change this to your desired path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spark = configure_spark('minio','main')\n",
    "\n",
    "# Path to the Parquet file\n",
    "#df = get_db_tables('hdr','2024-08-02',spark)\n",
    "local_table_name = \"spark_catalog.default.employees\"\n",
    "df = spark.read.format(\"iceberg\").table(local_table_name)\n",
    "\n",
    "\n",
    "# Convert column headers to uppercase (optional)\n",
    "#df = df.select([col(column).alias(column.upper()) for column in df.columns])\n",
    "\n",
    "# Define the table name\n",
    "table_name = \"nessie.employees\"\n",
    "\n",
    "# Define the table schema with backticks around column names\n",
    "schema = ', '.join([f'{field.name} {field.dataType.simpleString()}' for field in df.schema.fields])\n",
    "\n",
    "def create_iceberg_table_if_not_exists(spark, table_name, df, partition_column):\n",
    "    \"\"\"Create an Iceberg table if it doesn't exist.\"\"\"\n",
    "    schema = ', '.join([f'`{field.name}` {field.dataType.simpleString()}' for field in df.schema.fields])\n",
    "    create_table_query = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            {schema}\n",
    "        )\n",
    "        USING iceberg\n",
    "        PARTITIONED BY ({partition_column})\n",
    "    \"\"\"\n",
    "    spark.sql(create_table_query)\n",
    "\n",
    "# Write the DataFrame to the existing Iceberg table partitioned by the 'DATE' column\n",
    "create_iceberg_table_if_not_exists(spark, table_name, df, 'companyName')\n",
    "df.write.format(\"iceberg\").mode(\"overwrite\") \\\n",
    "    .save(table_name)\n",
    "\n",
    "print(f\"Data has been written to Iceberg table at {iceberg_warehouse_path}/default/{table_name}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "from utils.utils import configure_spark\n",
    "\n",
    "def create_iceberg_table_if_not_exists(spark, table_name, df, partition_column):\n",
    "    \"\"\"Create an Iceberg table if it doesn't exist.\"\"\"\n",
    "    schema = ', '.join([f'`{field.name}` {field.dataType.simpleString()}' for field in df.schema.fields])\n",
    "    create_table_query = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            {schema}\n",
    "        )\n",
    "        USING iceberg\n",
    "        PARTITIONED BY ({partition_column})\n",
    "    \"\"\"\n",
    "    spark.sql(create_table_query)\n",
    "\n",
    "def query_and_write_to_local_table(spark, parquet_path, local_table_name, partition_column):\n",
    "    \"\"\"Query data from Nessie table for a given date and write it to the local Iceberg table.\"\"\"\n",
    "    df = spark.read.parquet(parquet_path)\n",
    "    create_iceberg_table_if_not_exists(spark, local_table_name, df, partition_column)\n",
    "    df.writeTo(local_table_name).append()\n",
    "\n",
    "def read_local_and_write_to_nessie(spark, local_table_name, nessie_table_name, partition_column):\n",
    "    \"\"\"Read data from local Iceberg table and write it to the Nessie-managed Iceberg table.\"\"\"\n",
    "    local_df = spark.read.format(\"iceberg\").table(local_table_name)\n",
    "    create_iceberg_table_if_not_exists(spark, nessie_table_name, local_df, partition_column)\n",
    "    local_df.writeTo(nessie_table_name).append()\n",
    "\n",
    "def process_data(spark, parquet_path, local_table_name,nessie_table_name,partition_column):\n",
    "    \"\"\"Process a list of dates to query from Nessie and write to both local and Nessie Iceberg tables.\"\"\"\n",
    "  \n",
    "    query_and_write_to_local_table(spark, parquet_path, local_table_name,partition_column)\n",
    "    #read_local_and_write_to_nessie(spark, local_table_name, nessie_table_name, partition_column)\n",
    "\n",
    "\n",
    "\n",
    "# Define the path for the Iceberg warehouse\n",
    "iceberg_warehouse_path = \"iceberg_warehouse\"\n",
    "os.makedirs(iceberg_warehouse_path, exist_ok=True)\n",
    "# Initialize Spark session with the configured SparkConf\n",
    "#spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "spark = configure_spark('minio','main')\n",
    "\n",
    "# Define the Nessie and local table names\n",
    "parquet_path = 'scripts/python_go_backups/employees_go.parquet'\n",
    "nessie_table_name = \"nessie.employees\"\n",
    "local_table_name = \"spark_catalog.default.employees\"\n",
    "partition_column = 'companyName'\n",
    "\n",
    "process_data(spark, parquet_path, local_table_name,nessie_table_name, partition_column)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "124_036_762"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import configure_spark\n",
    "import pandas as pd\n",
    "\n",
    "local_table_name = \"spark_catalog.default.employees\"\n",
    "spark = configure_spark('minio','main')\n",
    "\n",
    "local_df = spark.read.format(\"iceberg\").table(local_table_name)\n",
    "local_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured MINIO environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/22 01:51:17 WARN Utils: Your hostname, vision resolves to a loopback address: 127.0.1.1; using 192.168.1.112 instead (on interface enxc8a36204def5)\n",
      "24/08/22 01:51:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/kdlocpanda/.ivy2/cache\n",
      "The jars for the packages stored in: /home/kdlocpanda/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12 added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-217553ac-1c97-4f62-b02d-2be6a199692e;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/kdlocpanda/yorko_io/open-datalakehouse/.venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 in central\n",
      "\tfound org.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.91.3 in central\n",
      "\tfound software.amazon.awssdk#bundle;2.17.81 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 126ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 from central in [default]\n",
      "\torg.projectnessie.nessie-integrations#nessie-spark-extensions-3.5_2.12;0.91.3 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.17.81 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   0   |   0   |   0   ||   7   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-217553ac-1c97-4f62-b02d-2be6a199692e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 7 already retrieved (0kB/4ms)\n",
      "24/08/22 01:51:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/22 01:51:18 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>companyName</th>\n",
       "      <th>employee_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tagcat-6285</td>\n",
       "      <td>485254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jaxworks</td>\n",
       "      <td>341507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jabberbean-6063</td>\n",
       "      <td>430670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Viva-2310</td>\n",
       "      <td>433971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jabbersphere-5488</td>\n",
       "      <td>187187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Skyble</td>\n",
       "      <td>259178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Zoozzy</td>\n",
       "      <td>202266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Dynava-3880</td>\n",
       "      <td>373082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Realcube-9666</td>\n",
       "      <td>376158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Rhyzio</td>\n",
       "      <td>195181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           companyName  employee_count\n",
       "0          Tagcat-6285          485254\n",
       "1             Jaxworks          341507\n",
       "2      Jabberbean-6063          430670\n",
       "3            Viva-2310          433971\n",
       "4    Jabbersphere-5488          187187\n",
       "..                 ...             ...\n",
       "995             Skyble          259178\n",
       "996             Zoozzy          202266\n",
       "997        Dynava-3880          373082\n",
       "998      Realcube-9666          376158\n",
       "999             Rhyzio          195181\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.utils import configure_spark\n",
    "\n",
    "local_table_name = \"spark_catalog.default.employees\"\n",
    "spark = configure_spark('minio','main')\n",
    "\n",
    "# Read the Iceberg table into a DataFrame\n",
    "local_df = spark.read.format(\"iceberg\").table(local_table_name)\n",
    "\n",
    "# Register the DataFrame as a temporary view\n",
    "local_df.createOrReplaceTempView(\"employees_view\")\n",
    "\n",
    "# Perform the query using Spark SQL\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT companyName, COUNT(*) as employee_count\n",
    "    FROM employees_view\n",
    "    GROUP BY companyName\n",
    "\"\"\")\n",
    "\n",
    "# Show the results\n",
    "result_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect('iceberg_data.duckdb')\n",
    "\n",
    "# Install and load the Iceberg extension\n",
    "con.execute(\"INSTALL iceberg;\")\n",
    "con.execute(\"LOAD iceberg;\")\n",
    "\n",
    "# Query the Iceberg table and save the results to a DuckDB table\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE employees AS\n",
    "    SELECT *\n",
    "    FROM iceberg_scan('iceberg_warehouse/default/employees', allow_moved_paths = true);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>companyName</th>\n",
       "      <th>employee_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zoombeat</td>\n",
       "      <td>499813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Voomm-3470</td>\n",
       "      <td>499707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Izio-4877</td>\n",
       "      <td>498717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oozz-8135</td>\n",
       "      <td>498524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Roomm</td>\n",
       "      <td>498292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Wordify</td>\n",
       "      <td>2055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Wikivu</td>\n",
       "      <td>1887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Bubblebox</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Edgeblab-9808</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Vinder</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       companyName  employee_count\n",
       "0         Zoombeat          499813\n",
       "1       Voomm-3470          499707\n",
       "2        Izio-4877          498717\n",
       "3        Oozz-8135          498524\n",
       "4            Roomm          498292\n",
       "..             ...             ...\n",
       "995        Wordify            2055\n",
       "996         Wikivu            1887\n",
       "997      Bubblebox            1225\n",
       "998  Edgeblab-9808             503\n",
       "999         Vinder              67\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect('iceberg_data.duckdb')\n",
    "\n",
    "# Query to get the count of employees by companyName and convert it to a pandas DataFrame\n",
    "df = con.execute(\"\"\"\n",
    "    SELECT companyName, COUNT(*) as employee_count\n",
    "    FROM employees\n",
    "    GROUP BY companyName ORDER BY employee_count DESC;\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc cp iceberg_data.duckdb myminio/upload/iceberg_data.duckdb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
